{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "#from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "f_3a4='backlog3a4-pcba_allocation.csv'\n",
    "f_supply='test_SCR+OH+Intransit_0924.xlsx'\n",
    "sheet_scr='scr'\n",
    "sheet_transit='in-transit'\n",
    "sheet_oh='OH & transit-time'\n",
    "\n",
    "ranking_col=['priority_rank', 'ossd_offset', 'fcd_offset','rev_non_rev_rank','C_UNSTAGED_QTY', 'SO_SS','PO_NUMBER']\n",
    "\n",
    "# backlog offset by transit pad will not consider ocean ship - assuming ocean is to cocver fcst demand but not backlog demand\n",
    "transit_time={'FOL':{'FOC':1,\n",
    "                     'FTX':7,\n",
    "                     'FCZ':9,\n",
    "                     'FJZ':10,\n",
    "                     'SJZ':10,\n",
    "                     'JMX':9,\n",
    "                     'FGU':9,\n",
    "                     'JPE':4,\n",
    "                     'JPI':5,\n",
    "                     'FSJ':6,\n",
    "                     'FE':9,\n",
    "                     'other':7}, \n",
    "            'JPE':{'JPE':1,'other':7}\n",
    "           }\n",
    "# within below days transit ETA considered as OH\n",
    "close_eta_cutoff_criteria=15\n",
    "# Far ETA eligible to backward fulfill PO: offset OSSD by deducting below days\n",
    "eta_backward_offset_days=15\n",
    "\n",
    "pcba_site='FOL'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_supply_to_versionless_and_addup_supply(df,org_col='planningOrg',pn_col='TAN'):\n",
    "    \"\"\"\n",
    "    Change PN in supply or OH df into versionless. Add up the qty into the versionless PN.\n",
    "    :param df: the supply df or oh df\n",
    "    :param pn_col: name of the PN col. In Cm supply file it's PN, in Kinaxis file it's TAN.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    regex = re.compile(r'\\d{2,3}-\\d{4,7}')\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # convert to versionless and add temp col\n",
    "    df.loc[:,pn_col] = df[pn_col].map(lambda x: regex.search(x).group())\n",
    "    df.loc[:,'org_pn']=df[org_col] + '_' + df[pn_col]\n",
    "\n",
    "    # add up the duplicate PN (due to multiple versions)\n",
    "    df.sort_values(by=['org_pn'],inplace=True)\n",
    "    dup_pn = df[df.duplicated(['org_pn'])]['org_pn'].unique()\n",
    "    df_sum = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    df_sum.set_index([org_col,pn_col,'org_pn'], inplace=True)\n",
    "    df.set_index([org_col,pn_col,'org_pn'], inplace=True)\n",
    "        \n",
    "    for org_pn in dup_pn:\n",
    "        # print(df_supply[df_supply.PN==pn].sum(axis=1).sum())\n",
    "        df_sum.loc[(org_pn[:3],org_pn[4:],org_pn), :] = df.loc[(org_pn[:3],org_pn[4:],org_pn), :].sum(axis=0)\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    df.set_index('org_pn',inplace=True)\n",
    "    df.drop(dup_pn, axis=0, inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.set_index([org_col,pn_col,'org_pn'], inplace=True)\n",
    "    #print(df.columns)\n",
    "    #df.drop(['level_0','index'],axis=1,inplace=True)\n",
    "    df = pd.concat([df, df_sum])\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(['org_pn'],axis=1,inplace=True)\n",
    "    df.set_index([org_col,pn_col], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_date_with_transit_pad(x,y,transit_time,pcba_site):\n",
    "    \"\"\"\n",
    "    offset transit time to a given date column\n",
    "    \"\"\"\n",
    "    if x in transit_time[pcba_site].keys():\n",
    "        return y - pd.Timedelta(days=transit_time[pcba_site][x])\n",
    "    else:\n",
    "        return y - pd.Timedelta(days=transit_time[pcba_site]['other'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df_order_bom_from_flb_tan_col(df_3a4,pcba):\n",
    "    \"\"\"\n",
    "    Generate the BOM usage file from the FLB_TAN col\n",
    "    :param df_3a4:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    regex_pn = re.compile(r'\\d{2,3}-\\d{4,7}')\n",
    "    regex_usage = re.compile(r'\\([0-9.]+\\)')\n",
    "\n",
    "    df_flb_tan = df_3a4[df_3a4.FLB_TAN.notnull()][['PO_NUMBER','PRODUCT_ID','ORDERED_QUANTITY','FLB_TAN']].copy()\n",
    "    #df_flb_tan.drop_duplicates(['PRODUCT_ID'], keep='first', inplace=True)\n",
    "\n",
    "    po_list=[]\n",
    "    pn_list = []\n",
    "    usage_list = []\n",
    "    for row in df_flb_tan.itertuples(index=False):\n",
    "        po=row.PO_NUMBER\n",
    "        flb_tan = row.FLB_TAN\n",
    "        #order_qty = row.ORDERED_QUANTITY\n",
    "        flb_tan=flb_tan.split('|')\n",
    "        \n",
    "        for item in flb_tan:\n",
    "            try:\n",
    "                pn = regex_pn.search(item).group()\n",
    "                usage = regex_usage.search(item).group()\n",
    "                usage = float(usage[1:-1])\n",
    "                \n",
    "                if pn in pcba:\n",
    "                    po_list.append(po)\n",
    "                    pn_list.append(pn)\n",
    "                    usage_list.append(usage)\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "                #print(po_list)\n",
    "        \n",
    "    #print(po_list)\n",
    "    df_order_bom_from_flb = pd.DataFrame({'PO_NUMBER': po_list, 'BOM_PN': pn_list, 'BOM_PN_QTY': usage_list})\n",
    "\n",
    "    return df_order_bom_from_flb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_order_bom_to_3a4(df_3a4, df_order_bom):\n",
    "    \"\"\"\n",
    "    Add PN into 3a4 based on BOM\n",
    "    :param df_3a4:\n",
    "    :param df_bom:\n",
    "    :return: df_3a4, df_missing_bom_pid\n",
    "    \"\"\"\n",
    "    # add the BOM PN through merge method\n",
    "    df_3a4 = pd.merge(df_3a4, df_order_bom, left_on='PO_NUMBER', right_on='PO_NUMBER', how='left')\n",
    "\n",
    "    \"\"\"\n",
    "    # PID missing BOM data\n",
    "    missing_bom_pid = df_3a4[df_3a4.TAN.notnull() & df_3a4.PN.isnull()].PRODUCT_ID.unique()\n",
    "    df_missing_bom_pid = pd.DataFrame({'Missing BOM PID': missing_bom_pid})\n",
    "\n",
    "    # 对于BOM missing 的采用3a4中已有的TAN\n",
    "    df_3a4.loc[:, 'PN'] = np.where(df_3a4.TAN.notnull() & df_3a4.PN.isnull(),\n",
    "                                   df_3a4.TAN,\n",
    "                                   df_3a4.PN)\n",
    "    \"\"\"\n",
    "    # correct the quantity by multiplying BOM Qty\n",
    "    df_3a4.loc[:, 'C_UNSTAGED_QTY']=df_3a4.C_UNSTAGED_QTY * (df_3a4.BOM_PN_QTY/df_3a4.ORDERED_QUANTITY)\n",
    "    df_3a4.loc[:, 'ORDERED_QUANTITY'] = df_3a4.BOM_PN_QTY\n",
    "\n",
    "    # add indicator for distinct PO filtering\n",
    "    df_3a4.loc[:,'distinct_po_filter']=np.where(~df_3a4.duplicated('PO_NUMBER'),\n",
    "                                              'YES',\n",
    "                                                '')\n",
    "\n",
    "\n",
    "    return df_3a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def created_supply_dict_per_scr(df_scr):\n",
    "    \"\"\"\n",
    "    create supply dict based on scr\n",
    "    supply_dic_tan={'800-42373':[{'2/10':25},{'2/12':4},{'2/15':10},{'2/22':20},{'3/1':10},{'3/5':15}],\n",
    "               '800-42925':[{'2/12':4},{'2/13':3},{'2/15':12},{'2/23':25},{'3/1':8},{'3/6':10}]}\n",
    "    \"\"\"\n",
    "    supply_dic_tan={}\n",
    "    \n",
    "    \n",
    "    for tan in df_scr.index:\n",
    "        date_qty_list=[]\n",
    "        for date in df_scr.columns:\n",
    "            date_qty={date:df_scr.loc[tan,date]}\n",
    "            if not math.isnan(df_scr.loc[tan,date]): # 判断数值是否为空\n",
    "                if df_scr.loc[tan,date]>0: # 不取0值\n",
    "                    date_qty_list.append(date_qty)\n",
    "        supply_dic_tan[tan]=date_qty_list\n",
    "        \n",
    "    \"\"\" Below version spends much logner time to create the dict. use above instead with index simplified.\n",
    "    for org_tan in df_scr.index:\n",
    "        org=org_tan[0]\n",
    "        tan=org_tan[1]\n",
    "        date_qty_list=[]\n",
    "        for date in df_scr.columns:\n",
    "            date_qty={date:df_scr.loc[(org,tan),date]}\n",
    "            if not math.isnan(df_scr.loc[(org,tan),date]): # 判断数值是否为空\n",
    "                if df_scr.loc[(org,tan),date]>0: # 不取0值\n",
    "                    date_qty_list.append(date_qty)\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    return supply_dic_tan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def created_oh_dict_per_df_oh(df_oh,pcba_site):\n",
    "    \"\"\"\n",
    "    (Also used for transit eta close dict)create OH dict based on DF OH (excluding PCBA site and only consider OH>0 items)\n",
    "    oh_dic_tan={(FOC,'800-42373'):25,(FJZ,'800-42925'):100}\n",
    "    \"\"\"\n",
    "    df_oh=df_oh[(df_oh.OH>0)]\n",
    "    df_oh.reset_index(inplace=True)\n",
    "    oh_dic_tan={}\n",
    "    for row in df_oh.itertuples(index=False):\n",
    "        org=row.planningOrg\n",
    "        tan=row.TAN\n",
    "        oh=row.OH\n",
    "        \n",
    "        if org!=pcba_site:\n",
    "            oh_dic_tan[(org,tan)]=oh\n",
    "    \n",
    "    return oh_dic_tan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transit_dict_per_df_transit(df_transit):\n",
    "    \"\"\"\n",
    "    Create transit dict based on df_transit_eta_late.\n",
    "    transit_dict_tan={(FOC,'800-42373'):(15,2020-10-20)}\n",
    "    \"\"\"\n",
    "    transit_dic_tan={}\n",
    "       \n",
    "    for org_tan in df_transit.index:\n",
    "        date_qty_list=[]\n",
    "        for date in df_transit.columns:          \n",
    "            if not math.isnan(df_transit.loc[(org_tan[0],org_tan[1]),date]): # 判断数值是否为空\n",
    "                if df_transit.loc[(org_tan[0],org_tan[1]),date]>0: # 不取0值\n",
    "                    date_qty={date:df_transit.loc[(org_tan[0],org_tan[1]),date]}\n",
    "                    date_qty_list.append(date_qty)\n",
    "        if len(date_qty_list)>0:\n",
    "            transit_dic_tan[(org_tan[0],org_tan[1])]=date_qty_list\n",
    "        \n",
    "        \n",
    "    return transit_dic_tan\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blg_dict_per_sorted_3a4_and_selected_tan(df_3a4,tan):\n",
    "    \"\"\"\n",
    "    create backlog dict for selected tan list from the sorted 3a4 df (considered order prioity and rank)\n",
    "    blg_ic_tan={'800-42373':{'FJZ':(5,'1234567-1','2020-10-20')}}\n",
    "    \"\"\"\n",
    "    blg_dic_tan={}\n",
    "    for pn in tan:\n",
    "        dfm=df_3a4[df_3a4.BOM_PN==pn]\n",
    "        org_qty_po=[]\n",
    "        for org,qty,po,ossd in zip(dfm.ORGANIZATION_CODE,dfm.BOM_PN_QTY,dfm.PO_NUMBER,dfm.ossd_offset):\n",
    "            if qty>0:\n",
    "                org_qty_po.append({org:(qty,po,ossd.date())})\n",
    "\n",
    "        blg_dic_tan[pn]=org_qty_po\n",
    "    \n",
    "    return blg_dic_tan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_supply_per_supply_and_blg_dic(supply_dic_tan,blg_dic_tan):\n",
    "    \"\"\"\n",
    "    allocate supply based on supply dict and backlog dict\n",
    "    supply dict is aranged in date order; backlog dict is aranged based on priority to fulfill\n",
    "    \n",
    "    examples: \n",
    "        blg_dic_tan={'800-42373-01': [{'FJZ': (5, '110077267-1')},{'FJZ': (23, '110011089-4')},...]}\n",
    "        supply_dic_tan={'800-42373-01':[{'2/10':25},{'2/12':4},{'2/15':10},{'2/22':20},{'3/1':10},{'3/5':15}],\n",
    "                             '800-42925-01':[{'2/12':4},{'2/13':3},{'2/15':12},{'2/23':25},{'3/1':8},{'3/6':10}]}\n",
    "    \"\"\"\n",
    "    supply_dic_tan_allocated={}\n",
    "    \n",
    "    for tan in supply_dic_tan.keys():\n",
    "        supply_list_tan=supply_dic_tan[tan]  #每一个tan对应的supply list\n",
    "\n",
    "        if tan in blg_dic_tan.keys(): #\n",
    "            blg_list_tan=blg_dic_tan[tan]\n",
    "\n",
    "            # 对supply list中每一个值进行分配给一个或多个订单\n",
    "            for date_qty in supply_list_tan:\n",
    "                #print(date_qty)\n",
    "                supply_date=list(date_qty.keys())[0]\n",
    "                supply_qty=list(date_qty.values())[0]\n",
    "                allocation=[] #每一个supply的分配结果\n",
    "\n",
    "                allocated_po=[] #已经分配给对应数量的po\n",
    "                # 对每一个po进行数量分配\n",
    "                for po in blg_list_tan:\n",
    "                    #print(po)\n",
    "                    po_qty=list(po.values())[0][0]\n",
    "                    po_org=list(po.keys())[0]\n",
    "                    po_number=list(po.values())[0][1]\n",
    "                    \n",
    "                    #print(po_qty)\n",
    "                    #print(supply_qty)\n",
    "                    if po_qty<supply_qty: #po数量小于supply数量：po被全额满足；supply数量被减掉；已分配的po被记录 （后面跳转到下一个po）\n",
    "                        allocation.append((po_org,po_qty))\n",
    "                        supply_qty=supply_qty-po_qty\n",
    "                        allocated_po.append(po)\n",
    "                    elif po_qty==supply_qty: #po数量等于supply数量：po被全额满足；已分配的po被记录；跳出本次po循环(进到下一个supply循环)\n",
    "                        allocation.append((po_org,po_qty))\n",
    "                        allocated_po.append(po)\n",
    "                        break\n",
    "                    else: #po数量大于supply数量：po被部分（=supply qty）满足；po数量被改小；跳出本次po循环(进到下一个supply循环)\n",
    "                        allocation.append((po_org,supply_qty))\n",
    "                        new_po_qty=po_qty-supply_qty\n",
    "                        ind=blg_list_tan.index(po)\n",
    "                        blg_list_tan[ind]={po_org:(new_po_qty,po_number)}\n",
    "                        break\n",
    "\n",
    "                #print(allocated_po)\n",
    "                # 把已经被分配的po从列表中删除\n",
    "                for po in allocated_po:\n",
    "                    #print(po)\n",
    "                    blg_list_tan.remove(po)   # double check this one of removing PO whether correct or not\n",
    "                    blg_dic_tan[tan]=blg_list_tan\n",
    "\n",
    "\n",
    "                # 把supply列表中对应的supply改变成分配的结果\n",
    "                ind=supply_list_tan.index(date_qty)\n",
    "                supply_date=list(date_qty.keys())[0]\n",
    "                supply_qty=list(date_qty.values())[0]\n",
    "                supply_list_tan[ind]={supply_date:(supply_qty,allocation)}\n",
    "\n",
    "        #生成新的allocated supply dict\n",
    "        supply_dic_tan_allocated[tan]=supply_list_tan\n",
    "    \n",
    "    return supply_dic_tan_allocated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_allocation_for_each_date(a,date_supply_agg):\n",
    "    \"\"\"\n",
    "    # 根据日期及org汇合每个日期dict下的数量 \n",
    "    a={'2/12': (4, [('FCZ', 1), ('FJZ', 1), ('FJZ', 1), ('FJZ', 1)])}\n",
    "    date_supply_agg={}\n",
    "    此函数在aggregate_supply_dic_tan_allocated中引用\n",
    "    \"\"\"\n",
    "    date=list(a.keys())[0]\n",
    "    supply=list(a.values())[0][1]  \n",
    "    supply_total_qty=list(a.values())[0][0]  \n",
    "    \n",
    "    orgs=[]\n",
    "    for org_supply in supply:\n",
    "        if org_supply[0] not in orgs:\n",
    "            orgs.append(org_supply[0])\n",
    "    \n",
    "    allocation_agg=[]\n",
    "    for org in orgs:\n",
    "        qty=0\n",
    "        for org_supply in supply:\n",
    "            if org_supply[0]==org:\n",
    "                qty+=org_supply[1]\n",
    "        \n",
    "        allocation_agg.append((org,qty))\n",
    "    \n",
    "    date_supply_agg={}\n",
    "    date_supply_agg[date]=(supply_total_qty,allocation_agg)\n",
    "    \n",
    "    return date_supply_agg\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_supply_dic_tan_allocated(supply_dic_tan_allocated):\n",
    "    \"\"\"\n",
    "    针对每一个tan按照每一个日期将分配的数量按照org汇总(引用函数aggregate_allocation_for_each_date)\n",
    "    \"\"\"\n",
    "    supply_dic_tan_allocated_agg={}\n",
    "    for tan,tan_supply in supply_dic_tan_allocated.items():\n",
    "        tan_supply_list=[]\n",
    "        for date_supply in tan_supply:\n",
    "            date_supply_agg={}\n",
    "            date_supply_agg=aggregate_allocation_for_each_date(date_supply,date_supply_agg)\n",
    "            tan_supply_list.append(date_supply_agg)\n",
    "\n",
    "        supply_dic_tan_allocated_agg[tan]=tan_supply_list\n",
    "    \n",
    "    return supply_dic_tan_allocated_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fulfill_backlog_by_oh(oh_dic_tan, blg_dic_tan):\n",
    "    \"\"\"\n",
    "    Fulfill the backlog per DF site based on the DF site OH; deduct the backlog qty accordingly.\n",
    "    examples:\n",
    "        blg_dic_tan={'800-42373': [{'FJZ': (5, '110077267-1','2020-4-1')},{'FJZ': (23, '110011089-4','2020-4-4')},...]}\n",
    "        oh_dic_tan={('FJZ',800-42373'):25,('FCZ',800-42925'):10}\n",
    "    return: blg_dic_tan\n",
    "    \"\"\"\n",
    "    for org_tan,qty in oh_dic_tan.items():\n",
    "        oh_org=org_tan[0]\n",
    "        oh_tan=org_tan[1]\n",
    "        oh_qty=qty\n",
    "        \n",
    "        if oh_tan in blg_dic_tan.keys():  # blg_dic_tan只包含scr中的tan，oh_tan可能不在其中，如不在，不予考虑\n",
    "            blg_dic_tan_list=blg_dic_tan[oh_tan] #对应tan下的内容\n",
    "            blg_dic_tan_list_copy=blg_dic_tan_list.copy()\n",
    "            \n",
    "            # 按顺序对每一个po进行数量分配\n",
    "            for org_po in blg_dic_tan_list:\n",
    "                po_org=list(org_po.keys())[0]\n",
    "                po_qty = list(org_po.values())[0][0]\n",
    "                po_number = list(org_po.values())[0][1]\n",
    "                po_ossd=list(org_po.values())[0][2]\n",
    "                \n",
    "                if po_org==oh_org:\n",
    "                    po_qty_new=po_qty-oh_qty\n",
    "                    oh_qty_new=oh_qty - po_qty\n",
    "                    if po_qty_new<=0: #po已被oh cover完，移除po\n",
    "                        blg_dic_tan_list_copy.remove(org_po)\n",
    "                        oh_qty=oh_qty_new\n",
    "                    else: # oh consumed\n",
    "                        index=blg_dic_tan_list_copy.index(org_po)\n",
    "                        blg_dic_tan_list_copy[index]={po_org:(po_qty_new,po_number,po_ossd)}\n",
    "                    \n",
    "                        break\n",
    "            # 更新blg_dic_tan\n",
    "            blg_dic_tan[oh_tan]=blg_dic_tan_list_copy\n",
    "                \n",
    "    return blg_dic_tan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_allocation_to_scr(df_scr,df_3a4,supply_dic_tan_allocated_agg,pcba_site):\n",
    "    \"\"\"\n",
    "    Add up the allocation results to scr and create the final output file\n",
    "    \"\"\"\n",
    "    pcba_site_temp='A-' + pcba_site\n",
    "    df_scr.loc[:,'ORG']=pcba_site_temp\n",
    "    df_scr.reset_index(inplace=True)\n",
    "    df_scr.set_index(['TAN','ORG'],inplace=True)\n",
    "    \n",
    "    # Add in orgs based on 3a4\n",
    "    df_3a4_p=df_3a4.pivot_table(index=['BOM_PN','ORGANIZATION_CODE'],values='PO_NUMBER',aggfunc=len)\n",
    "    df_3a4_p.reset_index(inplace=True)\n",
    "    \n",
    "    for row in df_3a4_p.itertuples():\n",
    "        tan=row.BOM_PN\n",
    "        org=row.ORGANIZATION_CODE\n",
    "        \n",
    "        if tan in df_scr.index:\n",
    "            df_scr.loc[(tan,org),'count']=row.PO_NUMBER\n",
    "    df_scr.drop('count',axis=1,inplace=True)\n",
    "    \n",
    "    # add in allocated qty\n",
    "    for tan in supply_dic_tan_allocated_agg.keys():\n",
    "        for date_supply in supply_dic_tan_allocated_agg[tan]:\n",
    "            date=list(date_supply.keys())[0]\n",
    "            org_qty=list(date_supply.values())[0][1]\n",
    "            for x in org_qty:\n",
    "                df_scr.loc[(tan,x[0]),date]=x[1]\n",
    "\n",
    "    df_scr.reset_index(inplace=True)\n",
    "    df_scr.sort_values(by=['TAN','ORG'],ascending=True,inplace=True)\n",
    "    df_scr.loc[:,'ORG']=df_scr.ORG.map(lambda x: pcba_site if 'A-' in x else x)\n",
    "    df_scr.set_index(['TAN','ORG'],inplace=True)\n",
    "    \n",
    "    return df_scr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bu_from_scr(df_scr):\n",
    "    \"\"\"\n",
    "    Versionless the PN and extract the BU info from original scr before pivoting\n",
    "    \"\"\"\n",
    "    regex_pn = re.compile(r'\\d{2,3}-\\d{4,7}')\n",
    "    \n",
    "    tan_bu={}\n",
    "    for row in df_scr.itertuples(index=False):\n",
    "        tan = regex_pn.search(row.TAN).group()\n",
    "        tan_bu[tan]=row.BU\n",
    "    \n",
    "    return tan_bu\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_final_allocated_output(df_scr,tan_bu,df_3a4,df_oh,df_transit,pcba_site):\n",
    "    \"\"\"\n",
    "    Add back the BU, backlog,oh, intransit info into the final SCR with allocation result; and add the related columns based on calculations.\n",
    "    \"\"\"\n",
    "    df_scr.reset_index(inplace=True)\n",
    "    \n",
    "    # add BU info\n",
    "    df_scr.loc[:,'BU']=df_scr.TAN.map(lambda x: tan_bu[x])\n",
    "    \n",
    "    # add backlog qty\n",
    "    df_3a4_p=df_3a4.pivot_table(index=['ORGANIZATION_CODE','BOM_PN'],values='BOM_PN_QTY',aggfunc=sum)\n",
    "    df_3a4_p.columns=['Backlog']\n",
    "    df_3a4_p.reset_index(inplace=True)\n",
    "    df_scr=pd.merge(df_scr,df_3a4_p,left_on=['ORG','TAN'],right_on=['ORGANIZATION_CODE','BOM_PN'],how='left')\n",
    "    \n",
    "    # add df OH\n",
    "    df_oh.columns=['OH']\n",
    "    df_oh.reset_index(inplace=True)\n",
    "    df_oh=df_oh[df_oh.planningOrg!=pcba_site]\n",
    "    df_scr=pd.merge(df_scr,df_oh,left_on=['ORG','TAN'],right_on=['planningOrg','TAN'],how='left')   \n",
    "    # drop the unneeded columns introduced by merge\n",
    "    df_scr.drop(['ORGANIZATION_CODE','BOM_PN','planningOrg'],axis=1,inplace=True)\n",
    "    #df_scr.rename(columns={'TAN_x':'TAN'},inplace=True)\n",
    "    \n",
    "    \n",
    "    # add df transit\n",
    "    df_transit.loc[:,'In-transit']=df_transit.sum(axis=1)\n",
    "    df_transit.reset_index(inplace=True)\n",
    "    df_scr=pd.merge(df_scr,df_transit[['planningOrg','TAN','In-transit']],left_on=['ORG','TAN'],right_on=['planningOrg','TAN'],how='left')\n",
    "    # drop the unneeded columns introduced by merge\n",
    "    df_scr.drop(['planningOrg'],axis=1,inplace=True)\n",
    "    \n",
    "    # ADD THE gap col and recovery date\n",
    "    df_scr.loc[:,'oh+transit']=df_scr.OH.fillna(0) + df_scr['In-transit'].fillna(0)\n",
    "    df_scr['oh+transit'].fillna(0,inplace=True)\n",
    "    df_scr.loc[:,'Gap_before']=np.where(df_scr.ORG!=pcba_site,\n",
    "                                        df_scr['oh+transit'] - df_scr.Backlog,\n",
    "                                        None)\n",
    "    df_scr.drop('oh+transit',axis=1,inplace=True)\n",
    "    \n",
    "    df_scr.loc[:,'Allocation']=np.where(df_scr.ORG!=pcba_site,\n",
    "                                         df_scr.iloc[:,3:-5].sum(axis=1),  # [3:-5] refer to the right data columns\n",
    "                                        None)\n",
    "                                                     \n",
    "    df_scr.loc[:,'Gap_after']=np.where(df_scr.ORG!=pcba_site,\n",
    "                                         df_scr.Gap_before+df_scr.Allocation,\n",
    "                                        None)\n",
    "    \n",
    "    df_scr.loc[:,'Recovery']=np.where((df_scr.ORG!=pcba_site),\n",
    "                                      np.where(df_scr.Gap_before>=0,\n",
    "                                               'No gap',\n",
    "                                              np.where(df_scr.Gap_after<0,\n",
    "                                                      'No recovery',\n",
    "                                                      'TBD')),\n",
    "                                     None)\n",
    "    \n",
    "    # update with the correct recovery date for TBD\n",
    "    df_scr.set_index(['TAN','ORG','BU','Backlog','OH','In-transit','Gap_before','Allocation','Gap_after','Recovery'],inplace=True)\n",
    "    df_scr.reset_index(inplace=True)\n",
    "    dfx=df_scr[(df_scr.Recovery=='TBD')&(df_scr.ORG!=pcba_site)]\n",
    "    dfx.set_index(['TAN','ORG'],inplace=True)\n",
    "    df_scr.set_index(['TAN','ORG'],inplace=True)\n",
    "    \n",
    "    for ind in dfx.index:\n",
    "        dfy=dfx.loc[ind,:]\n",
    "        dfy=dfy[dfy.notnull()]\n",
    "            \n",
    "        last_allocation_date=dfy.index[-1]\n",
    "        df_scr.loc[ind,'Recovery']=last_allocation_date\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    df_scr.reset_index(inplace=True)\n",
    "    df_scr.set_index(['TAN','ORG','BU','Backlog','OH','In-transit','Gap_before','Allocation','Gap_after','Recovery'],inplace=True)\n",
    "    \n",
    "    return df_scr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss_ranking_overall_new(df_3a4,ranking_col, order_col='SO_SS', new_col='ss_overall_rank'):\n",
    "    \"\"\"\n",
    "    根据priority_cat,OSSD,FCD, REVENUE_NON_REVENUE,C_UNSTAGED_QTY,按照ranking_col的顺序对SS进行排序。最后放MFG_HOLD订单.\n",
    "    :param df_3a4:\n",
    "    :param ranking_col:e.g. ['priority_rank', 'ORIGINAL_FCD_NBD_DATE', 'CURRENT_FCD_NBD_DATE','rev_non_rev_rank',\n",
    "                        'C_UNSTAGED_QTY', 'SO_SS','PO_NUMBER']\n",
    "    :param order_col:'SO_SS'\n",
    "    :param new_col:'ss_overall_rank'\n",
    "    :return: df_3a4\n",
    "    \"\"\"\n",
    "    # Below create a rev_rank for reference -  currently not used in overall ranking\n",
    "    ### change non-rev orders unstaged $ to 0\n",
    "    df_3a4.loc[:,'C_UNSTAGED_DOLLARS']=np.where(df_3a4.REVENUE_NON_REVENUE == 'NO',\n",
    "                                                0,\n",
    "                                                df_3a4.C_UNSTAGED_DOLLARS)\n",
    "\n",
    "    #### 生成ss_unstg_rev并据此排序\n",
    "    # 计算ss_unstg_rev\n",
    "    ss_unstg_rev = {}\n",
    "    df_rev = df_3a4.pivot_table(index='SO_SS', values='C_UNSTAGED_DOLLARS', aggfunc=sum)\n",
    "    for ss, rev in zip(df_rev.index, df_rev.values):\n",
    "        ss_unstg_rev[ss] = rev[0]\n",
    "    df_3a4.loc[:, 'ss_unstg_rev'] = df_3a4.SO_SS.map(lambda x: ss_unstg_rev[x])\n",
    "\n",
    "    \"\"\"\n",
    "    # 计算po_rev_unit - non revenue change to 0\n",
    "    df_3a4.loc[:, 'po_rev_unit'] = np.where(df_3a4.REVENUE_NON_REVENUE == 'YES',\n",
    "                                            df_3a4.SOL_REVENUE / df_3a4.ORDERED_QUANTITY,\n",
    "                                            0)\n",
    "\n",
    "    # 计算ss_rev_unit: 通过po_rev_unit汇总\n",
    "    ss_rev_unit = {}\n",
    "    dfx_rev = df_3a4.pivot_table(index='SO_SS', values='po_rev_unit', aggfunc=sum)\n",
    "    for ss, rev in zip(dfx_rev.index, dfx_rev.values):\n",
    "        ss_rev_unit[ss] = rev[0]\n",
    "    df_3a4.loc[:, 'ss_rev_unit'] = df_3a4.SO_SS.map(lambda x: int(ss_rev_unit[x]))\n",
    "    \"\"\"\n",
    "\n",
    "    # create rank#\n",
    "    rank = {}\n",
    "    order_list = df_3a4.sort_values(by='ss_unstg_rev', ascending=False).SO_SS.unique()\n",
    "    for order, rk in zip(order_list, range(1, len(order_list) + 1)):\n",
    "        rank[order] = rk\n",
    "    df_3a4.loc[:, 'ss_rev_rank'] = df_3a4.SO_SS.map(lambda x: rank[x])\n",
    "\n",
    "    # below creates overall ranking col\n",
    "    ### Step1: 重新定义priority order及排序\n",
    "    df_3a4.loc[:, 'priority_cat'] = np.where(df_3a4.SECONDARY_PRIORITY.isin(['PR1', 'PR2', 'PR3']),\n",
    "                                             df_3a4.SECONDARY_PRIORITY,\n",
    "                                             np.where(df_3a4.FINAL_ACTION_SUMMARY == 'TOP 100',\n",
    "                                                      'TOP 100',\n",
    "                                                      np.where(\n",
    "                                                          df_3a4.FINAL_ACTION_SUMMARY == 'LEVEL 4 ESCALATION PRESENT',\n",
    "                                                          'L4',\n",
    "                                                          np.where(df_3a4.BUP_RANK.notnull(),\n",
    "                                                                   'BUP',\n",
    "                                                                   None)\n",
    "                                                          )\n",
    "                                                      )\n",
    "                                             )\n",
    "    #### Update below DO/DX orders to PR1 due to current PR1/2/3 not updated when order change to DPAS from others\n",
    "    df_3a4.loc[:, 'priority_cat']=np.where((df_3a4.DPAS_RATING.isin(['DO','DX','TAA-DO','TAA-DX']))&(df_3a4.priority_cat.isnull()),\n",
    "                                           'PR1',\n",
    "                                           df_3a4.priority_cat)\n",
    "    #### Give them a rank\n",
    "    df_3a4.loc[:, 'priority_rank'] = np.where(df_3a4.priority_cat=='PR1',\n",
    "                                            1,\n",
    "                                            np.where(df_3a4.priority_cat =='PR2',\n",
    "                                                     2,\n",
    "                                                     np.where(df_3a4.priority_cat =='PR3',\n",
    "                                                              3,\n",
    "                                                              np.where(df_3a4.priority_cat == 'TOP 100',\n",
    "                                                                        4,\n",
    "                                                                        np.where(df_3a4.priority_cat == 'L4',\n",
    "                                                                                5,\n",
    "                                                                                np.where(df_3a4.priority_cat=='BUP',\n",
    "                                                                                         6,\n",
    "                                                                                         None)\n",
    "                                                                                )\n",
    "                                                                        )\n",
    "                                                                )\n",
    "                                                     )\n",
    "                                              )\n",
    "\n",
    "    ##### Step2: Give revenue/non-revenue a rank\n",
    "    df_3a4.loc[:,'rev_non_rev_rank']=np.where(df_3a4.REVENUE_NON_REVENUE=='YES', 0, 1)\n",
    "\n",
    "    ##### Step3: sort the SS per ranking columns and Put MFG hold orders at the back\n",
    "    df_3a4.sort_values(by=ranking_col, ascending=True, inplace=True)\n",
    "    # Put MFG hold orders at the back\n",
    "    df_hold=df_3a4[df_3a4.MFG_HOLD=='Y'].copy()\n",
    "    df_3a4=df_3a4[df_3a4.MFG_HOLD!='Y'].copy()\n",
    "    df_3a4=pd.concat([df_3a4,df_hold],sort=False)\n",
    "\n",
    "    ##### Step3: create rank# and put in 3a4\n",
    "    rank = {}\n",
    "    order_list = df_3a4[order_col].unique()\n",
    "    for order, rk in zip(order_list, range(1, len(order_list) + 1)):\n",
    "        rank[order] = rk\n",
    "    df_3a4.loc[:, new_col] = df_3a4[order_col].map(lambda x: rank[x])\n",
    "\n",
    "    return df_3a4\n",
    "\n",
    "\n",
    "def fulfill_backlog_by_transit_eta_late(transit_dic_tan, blg_dic_tan):\n",
    "    \"\"\"\n",
    "    Fulfill the backlog per DF site based on the DF site transit that is ETA far out; deduct the backlog qty accordingly.\n",
    "    examples:\n",
    "        blg_dic_tan={'800-42373': [{'FJZ': (5, '110077267-1','2020-4-1')},{'FJZ': (23, '110011089-4','2020-4-4')},...]}\n",
    "        transit_dic_tan={('FJZ',800-42373'):[{'2020-10-27':25},{'2020-10-29':10}]}\n",
    "    return: blg_dic_tan\n",
    "    \"\"\"\n",
    "    \n",
    "    for org_tan,date_qty_list in transit_dic_tan.items():\n",
    "        date_qty_list_copy=date_qty_list.copy()\n",
    "        transit_org=org_tan[0]\n",
    "        transit_tan=org_tan[1]\n",
    "        for date_qty in date_qty_list:   \n",
    "            transit_eta=list(date_qty.keys())[0]\n",
    "            transit_qty=list(date_qty.values())[0]\n",
    "            # backward offset the eta so to cover more ossd earlier than ETA\n",
    "            eta_backward_offset=transit_eta - pd.Timedelta(days=eta_backward_offset_days)\n",
    "            \n",
    "            if transit_tan in blg_dic_tan.keys():  # blg_dic_tan只包含scr中的tan，oh_tan可能不在其中，如不在，不予考虑\n",
    "                blg_dic_tan_list=blg_dic_tan[transit_tan] #对应tan下的内容\n",
    "                blg_dic_tan_list_copy=blg_dic_tan_list.copy()\n",
    "                # 按顺序对每一个po进行数量分配\n",
    "                for org_po in blg_dic_tan_list:\n",
    "                    po_org=list(org_po.keys())[0]\n",
    "                    po_qty = list(org_po.values())[0][0]\n",
    "                    po_number = list(org_po.values())[0][1]\n",
    "                    po_ossd=list(org_po.values())[0][2]\n",
    "                    \n",
    "                    if po_org==transit_org:\n",
    "                        if eta_backward_offset<=po_ossd or pd.isnull(po_ossd):# 考虑ossd,forward consumption\n",
    "                            po_qty_new=po_qty-transit_qty\n",
    "\n",
    "                            if po_qty_new<0: #po已被transit cover完，移除po;更新transit qty\n",
    "                                blg_dic_tan_list_copy.remove(org_po)\n",
    "                                index=date_qty_list_copy.index({transit_eta:transit_qty})\n",
    "                                transit_qty=transit_qty - po_qty\n",
    "                                date_qty_list_copy[index]={transit_eta:transit_qty}\n",
    "                            elif po_qty_new>0: # transit consumed by PO, 更新PO qty;移除transit\n",
    "                                index=blg_dic_tan_list_copy.index(org_po)\n",
    "                                blg_dic_tan_list_copy[index]={po_org:(po_qty_new,po_number,po_ossd)}\n",
    "                                date_qty_list_copy.remove({transit_eta:transit_qty})                    \n",
    "                                break\n",
    "                            else:         # po = transit, both are consumed\n",
    "                                blg_dic_tan_list_copy.remove(org_po)\n",
    "                                date_qty_list_copy.remove({transit_eta:transit_qty})\n",
    "                                break\n",
    "                \n",
    "                # 更新blg_dic_tan\n",
    "                blg_dic_tan[transit_tan]=blg_dic_tan_list_copy\n",
    "                \n",
    "        # 更新transit_dic_tan\n",
    "        transit_dic_tan[org_tan]=date_qty_list_copy\n",
    "                \n",
    "    return blg_dic_tan,transit_dic_tan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read 3a4, OH, Intransit, scr\n",
    "- Create Oh dict\n",
    "- Create intransit dict - how to deal with ETA TBD\n",
    "- Process 3a4\n",
    "- 3A4 consume DF OH by site\n",
    "- 3A4 consume intransit - how to deal with ETA TBD\n",
    "- Rank 3A4\n",
    "- Create scr dict\n",
    "- Allocate scr to 3a4\n",
    "- Summarize allocation result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 3a4\n",
    "df_3a4=pd.read_csv(f_3a4, encoding='ISO-8859-1',parse_dates=['CURRENT_FCD_NBD_DATE',  'ORIGINAL_FCD_NBD_DATE'],low_memory=False)\n",
    "\n",
    "\n",
    "# read scr\n",
    "df_scr=pd.read_excel(f_supply,sheet_name=sheet_scr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read oh this includes PCBA SM, will be removed when creating DF OH dict\n",
    "df_oh=pd.read_excel(f_supply,sheet_name=sheet_oh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in-transit\n",
    "df_transit=pd.read_excel(f_supply,sheet_name=sheet_transit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract BU info for TAN from SCR\n",
    "tan_bu=extract_bu_from_scr(df_scr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot df_scr 并处理日期格式\n",
    "df_scr=df_scr.pivot_table(index=['planningOrg','TAN'],columns='SCRDate',values='SCRQuantity',aggfunc=sum)\n",
    "df_scr.columns=df_scr.columns.map(lambda x: x.date())\n",
    "\n",
    "# versionless df_scr\n",
    "df_scr=change_supply_to_versionless_and_addup_supply(df_scr,pn_col='TAN')\n",
    "\n",
    "# simplify the index will make it much faster to get the dict - drop org and can add back later since know it's pcba_site\n",
    "df_scr.reset_index(inplace=True)\n",
    "df_scr.drop('planningOrg',axis=1,inplace=True)\n",
    "df_scr.set_index('TAN',inplace=True)\n",
    "supply_dic_tan=created_supply_dict_per_scr(df_scr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset 3A4 OSSD and FCD by transit time\n",
    "df_3a4.loc[:,'fcd_offset']=df_3a4[['ORGANIZATION_CODE','CURRENT_FCD_NBD_DATE']].apply(lambda x: update_date_with_transit_pad(x.ORGANIZATION_CODE, x.CURRENT_FCD_NBD_DATE,transit_time,pcba_site),axis=1)\n",
    "df_3a4.loc[:,'ossd_offset']=df_3a4.apply(lambda x: update_date_with_transit_pad(x.ORGANIZATION_CODE, x.ORIGINAL_FCD_NBD_DATE,transit_time,pcba_site),axis=1)\n",
    "\n",
    "\n",
    "# Rank the orders\n",
    "df_3a4=ss_ranking_overall_new(df_3a4,ranking_col, order_col='SO_SS', new_col='ss_overall_rank')\n",
    "\n",
    "\n",
    "# (do below after ranking) Process 3a4 BOM base on FLB_TAN col\n",
    "df_bom = generate_df_order_bom_from_flb_tan_col(df_3a4,supply_dic_tan.keys())\n",
    "df_3a4 = update_order_bom_to_3a4(df_3a4, df_bom)\n",
    "\n",
    "# create backlog dict for Tan exists in SCR\n",
    "blg_dic_tan=create_blg_dict_per_sorted_3a4_and_selected_tan(df_3a4,supply_dic_tan.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot df_oh\n",
    "df_oh=df_oh.pivot_table(index=['planningOrg','TAN'],values='OH',aggfunc=sum)\n",
    "\n",
    "# versionless df_oh\n",
    "df_oh=change_supply_to_versionless_and_addup_supply(df_oh,pn_col='TAN')\n",
    "\n",
    "# 生成OH dict；\n",
    "oh_dic_tan=created_oh_dict_per_df_oh(df_oh,pcba_site)\n",
    "\n",
    "# Oh to fulfill backlog per site. update blg_dic_tan accordingly\n",
    "blg_dic_tan=fulfill_backlog_by_oh(oh_dic_tan, blg_dic_tan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCZ (109.0, '111047139-2', datetime.date(2020, 9, 27))\n",
      "FCZ (2.0, '111095688-4', datetime.date(2020, 9, 8))\n",
      "FCZ (2.0, '111102545-1', datetime.date(2020, 9, 8))\n",
      "FCZ (1.0, '110981191-1', datetime.date(2020, 9, 12))\n",
      "FCZ (6.0, '111111317-1', datetime.date(2020, 9, 15))\n",
      "FCZ (2.0, '111116666-1', datetime.date(2020, 9, 15))\n",
      "FCZ (12.0, '111078965-1', datetime.date(2020, 9, 20))\n",
      "FCZ (2.0, '111102211-2', datetime.date(2020, 9, 20))\n",
      "FCZ (10.0, '111087560-1', datetime.date(2020, 9, 20))\n",
      "FCZ (10.0, '111087601-1', datetime.date(2020, 9, 20))\n",
      "FCZ (2.0, '111139562-1', datetime.date(2020, 9, 21))\n",
      "FCZ (12.0, '111150025-85', datetime.date(2020, 9, 21))\n",
      "FCZ (10.0, '111148648-2', datetime.date(2020, 9, 21))\n",
      "FCZ (2.0, '111147199-1', datetime.date(2020, 9, 22))\n",
      "FCZ (2.0, '111161425-4', datetime.date(2020, 9, 22))\n",
      "FCZ (1.0, '111170114-1', datetime.date(2020, 9, 22))\n",
      "FCZ (1.0, '111172939-6', datetime.date(2020, 9, 22))\n",
      "FCZ (20.0, '111179132-9', datetime.date(2020, 9, 22))\n",
      "FCZ (2.0, '111176618-1', datetime.date(2020, 9, 23))\n",
      "FCZ (2.0, '111142089-16', datetime.date(2020, 9, 26))\n",
      "FCZ (9.0, '110831615-6', datetime.date(2020, 9, 26))\n",
      "FCZ (2.0, '110831615-7', datetime.date(2020, 9, 26))\n",
      "FCZ (16.0, '111131485-2', datetime.date(2020, 9, 27))\n",
      "FCZ (2.0, '110873105-6', datetime.date(2020, 9, 29))\n",
      "FCZ (2.0, '110860261-1', datetime.date(2020, 9, 30))\n",
      "FCZ (2.0, '111103950-4', datetime.date(2020, 10, 3))\n",
      "FCZ (15.0, '110874244-1', datetime.date(2020, 10, 4))\n",
      "FCZ (1.0, '110868835-8', datetime.date(2020, 10, 5))\n",
      "FCZ (1.0, '111170136-2', datetime.date(2020, 10, 6))\n",
      "FCZ (10.0, '111141183-10', datetime.date(2020, 10, 6))\n",
      "FCZ (54.0, '111185952-1', datetime.date(2020, 10, 6))\n",
      "FCZ (2.0, '111162526-2', datetime.date(2020, 10, 10))\n",
      "FCZ (2.0, '111192890-2', datetime.date(2020, 10, 17))\n",
      "FCZ (46.0, '111192873-1', datetime.date(2020, 10, 17))\n",
      "FCZ (50.0, '111185972-1', datetime.date(2020, 10, 17))\n",
      "FCZ (2.0, '111193968-1', datetime.date(2020, 10, 24))\n",
      "FCZ (3.0, '111193578-1', datetime.date(2020, 10, 27))\n",
      "FCZ (4.0, '110997305-4', datetime.date(2020, 11, 2))\n",
      "FCZ (2.0, '111146524-10', datetime.date(2020, 11, 30))\n",
      "FCZ (12.0, '110877785-2', datetime.date(2020, 10, 4))\n",
      "FCZ (2.0, '111105260-11', NaT)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "for a in blg_dic_tan['68-101194']:\n",
    "    for x,y in a.items():\n",
    "        if x=='FCZ':\n",
    "            print(x,y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in-transit\n",
    "df_transit=pd.read_excel(f_supply,sheet_name=sheet_transit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot df_transit\n",
    "df_transit=df_transit.pivot_table(index=['planningOrg','TAN'],columns='ETA_date',values='In-transit_quantity',aggfunc=sum)\n",
    "df_transit.columns=df_transit.columns.map(lambda x: x.date())\n",
    "\n",
    "# versionless df_oh\n",
    "df_transit=change_supply_to_versionless_and_addup_supply(df_transit,pn_col='TAN')\n",
    "\n",
    "# split df_transit by threshhold of 15 days\n",
    "close_eta_cutoff=pd.Timestamp.today().date()+ pd.Timedelta(days=close_eta_cutoff_criteria)\n",
    "col=df_transit.columns\n",
    "df_transit_eta_early=df_transit.loc[:,col<=close_eta_cutoff].copy()\n",
    "df_transit_eta_early.loc[:,'OH']=df_transit_eta_early.sum(axis=1) # sum up as OH so can use the oh dict function\n",
    "df_transit_eta_late=df_transit.loc[:,col>close_eta_cutoff].copy()\n",
    "\n",
    "# 生成transit dict - for ETA close data use the OH dict function instead\n",
    "transit_dic_tan_eta_early=created_oh_dict_per_df_oh(df_transit_eta_early,pcba_site)\n",
    "transit_dic_tan_eta_late=create_transit_dict_per_df_transit(df_transit_eta_late)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 按照org将in-transit分配给自己的订单（forward consumption considering ETA per OSSD - ETA consider backward offset）\n",
    "#并更新blg_dic_tan\n",
    "blg_dic_tan=fulfill_backlog_by_oh(transit_dic_tan_eta_early, blg_dic_tan)\n",
    "blg_dic_tan,transit_dic_tan_eta_late=fulfill_backlog_by_transit_eta_late(transit_dic_tan_eta_late, blg_dic_tan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate SCR and 生成allocated supply dict\n",
    "supply_dic_tan_allocated=allocate_supply_per_supply_and_blg_dic(supply_dic_tan,blg_dic_tan)\n",
    "\n",
    "# TODO: (enhancement) if transit_dic_tan not consumed, come back to judge if the allocation is needed or not(per ETA) - if not, take it back and allocate to others\n",
    "\n",
    "\n",
    "#生成聚合的allocated supply dict\n",
    "supply_dic_tan_allocated_agg=aggregate_supply_dic_tan_allocated(supply_dic_tan_allocated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在df_scr中加入allocation结果\n",
    "df_scr=add_allocation_to_scr(df_scr,df_3a4,supply_dic_tan_allocated_agg,pcba_site)\n",
    "\n",
    "#把以下信息加回scr: BU, backlog, OH, intransit; 并做相应的计算处理\n",
    "df_scr=process_final_allocated_output(df_scr,tan_bu,df_3a4,df_oh,df_transit,pcba_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scr.to_excel('scr with allocation.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
